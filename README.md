# Ultra-Range Gesture Recognition 

This repository is based on the paper "Ultra-Range Gesture Recognition using a 
Web-Camera in Human-Robot Interaction". CLick here for the full article: 

Hand gestures are crucial for human interactions and can effectively convey 
commands in Human-Robot Interaction (HRI). This work introduces the Ultra-Range 
Gesture Recognition (URGR) framework, which extends recognition up to 25 meters 
using a deep-learning model with a simple RGB camera.

As a part of our work to comparing models, this repository employs a simple 
Hourglass model to improve the image quality of a long distance images, 
and a DenseNet model to classify them by the captured gesture. More well-known classification models were used in order 
to benchmark our prime model from the paper.


[Watch the video](https://www.youtube.com/watch?v=dw8BTe6PuDc&t=306s)


## ðŸ“œ Citation
**Eran Bamani and Eden Nissinman**, *Ultra-Range Gesture Recognition using a 
Web-Camera in Human-Robot Interaction* in **Engineering Applications of Artificial Intelligence**, 2024.
[Read the full paper](https://www.sciencedirect.com/science/article/pii/S0952197624006018)

### BibTeX:
If you use this work, please cite:
```bibtex
@article{BAMANI2024108443,
title = {Ultra-Range Gesture Recognition using a web-camera in Humanâ€“Robot Interaction},
journal = {Engineering Applications of Artificial Intelligence},
year = {2024},
doi = {https://doi.org/10.1016/j.engappai.2024.108443},
author = {Eran Bamani and Eden Nissinman and Inbar Meir and Lisa Koenigsberg and Avishai Sintov},
}